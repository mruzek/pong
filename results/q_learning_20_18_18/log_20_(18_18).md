charts for qlearning with epsilon decay cfg
  # grid_size: int = 20
  # goal: Tuple[int, int] = (18, 18)
  # step_reward: int = 0
  # alpha: float = range
  # gamma: float = range
  # epsilon: float = 1.0
  # epsilon_decay: float = 0.99
  # min_epsilon: float = 0.01
  # episodes: int = range
  # learning_rate: float = 0.1

---
Success with moderate-to-high alpha (0.3-0.7) and low-to-moderate gamma (0.1-0.3) suggests a problem where quick adaptation to information is helpful and where immediate or near-term rewards are more significant than distant future rewards.

Success at lower gamma:
  The problem involves immediate rewards that are more important than future rewards
  The optimal solution prioritizes short-term gains over long-term planning
  The environment might have shorter episodes or critical decision points occur early
  There may be less benefit to thinking many steps ahead immediate rewards

Success at higher gamma:
  Long-term planning is crucial
  Future rewards are as important as or more important than immediate rewards
  The problem likely requires considering consequences several steps ahead
  Delayed rewards are significant to the overall solution

Success at lower alpha:
  The environment might be noisy or stochastic
  Small, careful updates work better than large corrections
  The agent needs to average information over many experiences
  Stable, gradual learning is more effective than quick adaptation

Success at higher alpha:
  The environment is likely more deterministic
  Quick adaptation to new information is beneficial
  The agent can trust individual experiences more strongly
  The problem may change quickly, requiring rapid learning

  That clarifies things! With only 500 episodes for a 20x20 grid, your agent doesn't have enough time to fully propagate values across the entire state space with higher gamma values.
  Here's what's likely happening:

  With low gamma (0.001-0.3), the agent focuses on immediate or near-immediate rewards. In a pathfinding context, this creates a "greedy" behavior that works well for simple paths.
  
  Higher gamma values (0.5+) require more training episodes to be effective. With just 500 episodes, the agent hasn't had enough time to properly propagate the distant reward signals backward through the state space.
  
  The 20x20 grid has 400 states, and finding an optimal path requires value information to propagate across many steps. With high gamma, this propagation is slower to stabilize without enough training episodes.
  
  The performance cliff around gamma 0.4 likely represents the point where the agent starts trying to account for long-term rewards but hasn't had enough episodes to make those estimates reliable.

  This isn't necessarily a flaw - it's showing you exactly how these parameters behave with limited training. If your goal is to intentionally throttle performance to see some failures, then your current setup is achieving that.
  If you were to increase the episode count significantly (perhaps to 5,000 or 10,000), you'd likely see better performance with higher gamma values as the agent would have time to properly propagate the reward signals throughout the state space.
    
Why Low Gamma (0.1-0.3) Works Best:
With only 100 episodes,
 there's insufficient time for reward values to propagate back
 from the goal if using higher discount factors
Lower gamma creates a steeper "reward gradient"
that helps the agent learn faster within limited episodes
The minimum path length is 36 steps (18 right + 18 down),
 so reward propagation with high gamma would take many episodes
